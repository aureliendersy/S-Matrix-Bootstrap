{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0583b314",
   "metadata": {},
   "source": [
    "The objective of this notebook is to do some simple hyperparamters optimization with a callback to Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188244c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import neptune.new as neptune\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR, ExponentialLR\n",
    "from torch import Generator\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import neptune.new.integrations.optuna as optuna_utils\n",
    "import optuna\n",
    "\n",
    "# Set the Neptune Logger variables\n",
    "NEPTUNE_API_TOKEN = os.environ.get('NEPTUNE_API_TOKEN')\n",
    "NEPTUNE_PROJECT = os.environ.get('NEPTUNE_ALIAS') + '/2to2scattering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad08d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2884b",
   "metadata": {},
   "source": [
    "##### Define the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "896aed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhiNet(nn.Module):\n",
    "    def __init__(self, ffnlayers, activation, fund_dom, device='cpu', bound_phi = math.pi, final_layer='Sigmoid'):\n",
    "        \"\"\"\n",
    "        Initialize the network with the number of FC layers, the activation function, and the device to use\n",
    "        \"\"\"\n",
    "        super(PhiNet, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.fund_dom = fund_dom\n",
    "        self.bound_phi = bound_phi\n",
    "        \n",
    "        if activation=='ReLU':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Activation function '{}' not supported. Supported activation functions are 'ReLU' and 'Tanh'\".format(activation))\n",
    "\n",
    "        \n",
    "        if final_layer=='Sigmoid':\n",
    "            self.final_layer = nn.Sigmoid()\n",
    "            self.final_l = 'Sigmoid'\n",
    "        elif final_layer == 'Tanh':\n",
    "            self.final_layer = nn.Tanh()\n",
    "            self.final_l = 'Tanh'\n",
    "        elif final_layer == 'Identity':\n",
    "            self.final_layer = nn.Identity()\n",
    "            self.final_l = 'Identity'\n",
    "        else:\n",
    "            raise ValueError(\"Final layer '{}' not supported. Supported final layers are 'Sigmoid', 'Tanh' and 'Identity'\".format(final_layer))\n",
    "\n",
    "        \n",
    "        # Create a list of layers from the parameters given. Add the appropriate activation function\n",
    "        self.layer_sizes = ffnlayers\n",
    "        layer_list = []\n",
    "        \n",
    "        for i, layer_size in enumerate(self.layer_sizes):\n",
    "            if i == 0:\n",
    "                layer_list.append(('layer_%d' % (i+1), nn.Linear(1, layer_size)))\n",
    "                layer_list.append(('activation_%d' % (i+1), self.activation))\n",
    "            else:\n",
    "                layer_list.append(('layer_%d' % (i+1), nn.Linear(self.layer_sizes[i-1], layer_size)))\n",
    "                layer_list.append(('activation_%d' %(i+1), self.activation))\n",
    "        \n",
    "        layer_list.append(('layer_%d' % (len(self.layer_sizes)+1), nn.Linear(self.layer_sizes[-1], 1)))\n",
    "        \n",
    "        # The last layer is sigmoid to constrain outputs between 0 and 1 or tanh for [-1,1]\n",
    "        layer_list.append(('final_layer', self.final_layer))\n",
    "            \n",
    "        \n",
    "        self.model = nn.Sequential(OrderedDict(layer_list))\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Network forward pass where the outputs for the phase are in [-pi/2, pi/2] or [-pi, pi]\n",
    "        \"\"\"\n",
    "        #x = (x+1)/2 # Normalize the the cosine input variable\n",
    "        x = self.model(x)\n",
    "        if self.final_l == 'Identity':\n",
    "            return x\n",
    "        const_mult = 2 if self.final_l == 'Sigmoid' else 1\n",
    "        if self.fund_dom:\n",
    "            x = torch.tensor(const_mult*math.pi/2, device=self.device)*x - torch.tensor((const_mult*math.pi- self.bound_phi)/2, device=self.device)\n",
    "        else:\n",
    "            x = torch.tensor(const_mult*math.pi, device=self.device)*x - torch.tensor(const_mult*math.pi- self.bound_phi, device=self.device)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ModuleNetManual(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for parametrizing the given differential cross section\n",
    "    \"\"\"\n",
    "    def __init__(self, scan_param=None):\n",
    "        super(ModuleNetManual, self).__init__()\n",
    "        self.scan_param = scan_param\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Network forward pass is given by a specific function. Can iterate on the overall scale if we want to\n",
    "        \"\"\"\n",
    "        \n",
    "        denom = self.scan_param if self.scan_param is not None else 1\n",
    "        return (-torch.pow(x,3) + torch.pow(x,2) + x + 1)/denom\n",
    "        #return torch.abs(3*x + 1)/denom\n",
    "        #return torch.pow(x,2)/denom\n",
    "\n",
    "class PhaseNetSolver():\n",
    "    \"\"\"\n",
    "    Main class for crafting the simulation\n",
    "    \"\"\"\n",
    "    def __init__(self, params_simu, phi_net, mod_net, optimizer, random_generator, scheduler, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the simulation with the networks, the loss function and the optimizer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.epochs = params_simu['epochs_num']\n",
    "        self.batch_size = params_simu['batch_size']\n",
    "        self.eval_points_num = params_simu['integral_points']\n",
    "        self.scaled_loss = params_simu['scaled_loss']\n",
    "        self.method_int = params_simu['method_int']\n",
    "        self.p_value = params_simu['p_value']\n",
    "        self.lambda_repulsive = params_simu['lambda_repulsive']\n",
    "        self.loss_func = params_simu['loss']\n",
    "        \n",
    "        if isinstance(phi_net, list):\n",
    "            self.multi_nets = [net.to(device) for net in phi_net]\n",
    "            self.phi_net = None\n",
    "            self.multiple_nets = True\n",
    "        else:\n",
    "            self.multi_nets = None\n",
    "            self.phi_net = phi_net.to(device)\n",
    "            self.multiple_nets = False\n",
    "        self.mod_net = mod_net.to(device)\n",
    "        self.device = device\n",
    "        self.rand_gen = random_generator\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "    \n",
    "    def z2(self, z, z1, phi):\n",
    "        \"\"\"\n",
    "        Compute the z2 parameter (formula in motivation)\n",
    "        Place the batch size on first dim, z1 on the second and phi on the third\n",
    "        \"\"\"\n",
    "        term1 = z.unsqueeze(dim=0).transpose(0, 1) * (z1.unsqueeze(dim=0))\n",
    "        term2 = torch.sqrt(1 - torch.square(z)).unsqueeze(dim=0).transpose(0, 1) * torch.sqrt(1 - torch.square(z1)).unsqueeze(dim=0)\n",
    "        return term1.unsqueeze(dim=2) + term2.unsqueeze(dim=2)*torch.cos(phi.unsqueeze(dim=0)).view(1,1,-1)\n",
    "    \n",
    "    def integral_approximator(self, z, method='trapz'):\n",
    "        \"\"\"\n",
    "        Approximate the value of the RHS of the integral equation.\n",
    "        Compute the grids of z1 and phi points for given z values and integrate over the grids\n",
    "        \"\"\"\n",
    "        \n",
    "        # Linear partition of the space\n",
    "        z1_points = torch.linspace(-1,1,steps=self.eval_points_num, device=self.device, requires_grad=False)\n",
    "        phi_points = torch.linspace(0, 2*math.pi, steps=self.eval_points_num, device=self.device, requires_grad=False)\n",
    "        dx_z1 = 2/(self.eval_points_num-1)\n",
    "        dx_phi = 2*math.pi/(self.eval_points_num-1)\n",
    "        \n",
    "        grid_points = self.mod_net(z1_points).view(1,-1,1)*self.mod_net(self.z2(z, z1_points, phi_points))*torch.cos(self.phi_net(z1_points.view(1,-1,1)) - self.phi_net(self.z2(z, z1_points, phi_points).unsqueeze(dim=-1)).squeeze())\n",
    "        \n",
    "        # Simple trapezoid exists in PyTorch\n",
    "        if method == 'trapz':\n",
    "            integral = torch.trapezoid(torch.trapezoid(grid_points, dx=dx_phi), dx=dx_z1)/(4*math.pi) \n",
    "        \n",
    "        return integral\n",
    "    \n",
    "    def loss_function(self, zsamples):\n",
    "        \"\"\"\n",
    "        Compute a loss based on the average residuals for the integral equation\n",
    "        \"\"\"\n",
    "        \n",
    "        # For each value of z we estimate the integral equation\n",
    "        target = torch.ones(zsamples.size(), device=self.device) if self.scaled_loss else self.mod_net(zsamples)*torch.sin(self.phi_net(zsamples.view(-1,1)).squeeze())\n",
    "        model_input = self.integral_approximator(zsamples, method=self.method_int)\n",
    "        \n",
    "        # If scaled then all targets should be 1 irrespective of the z point considered\n",
    "        if self.scaled_loss:\n",
    "             model_input = model_input / (self.mod_net(zsamples)*torch.sin(self.phi_net(zsamples.view(-1,1)).squeeze()))\n",
    "            \n",
    "        # We have different choices of losses. Huber is more stable to outliers.\n",
    "        if self.loss_func =='Huber':\n",
    "            loss_out = F.huber_loss(model_input, target, delta=0.1)\n",
    "        elif self.loss_func == 'MSE':\n",
    "            loss_out = F.mse_loss(model_input, target)\n",
    "        else:\n",
    "            raise NotImplemented\n",
    "        \n",
    "        return loss_out\n",
    "    \n",
    "    def multi_loss_function(self, zsamples, logger=None):\n",
    "        \n",
    "        if not self.multiple_nets:\n",
    "            raise NotImplemented\n",
    "        \n",
    "        total_loss = 0\n",
    "        for i, net in enumerate(self.multi_nets):\n",
    "            self.phi_net = net\n",
    "            individual_loss = self.loss_function(zsamples)\n",
    "            \n",
    "            if logger is not None:\n",
    "                logger['metrics/solution_loss_{}'.format(i)].log(individual_loss)\n",
    "            total_loss = total_loss + individual_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def dual_loss(self, zsamples):\n",
    "             \n",
    "        if not self.multiple_nets and len(self.multi_nets)!=2:\n",
    "            raise NotImplemented\n",
    "        target = torch.stack((torch.cos(self.multi_nets[0](zsamples.view(-1,1)).squeeze()), torch.sin(self.multi_nets[0](zsamples.view(-1,1)).squeeze())))\n",
    "        model_input = torch.stack((torch.cos(self.multi_nets[1](zsamples.view(-1,1)).squeeze()), torch.sin(self.multi_nets[1](zsamples.view(-1,1)).squeeze())))\n",
    "        \n",
    "        # Maybe MSE makes more sense here since we are just comparing functions in the first place\n",
    "        loss_out = torch.pow(F.mse_loss(model_input, target), -self.p_value)\n",
    "        \n",
    "        return loss_out\n",
    "    \n",
    "    def dual_loss_ambiguity(self, zsamples):\n",
    "             \n",
    "        if not self.multiple_nets and len(self.multi_nets)!=2:\n",
    "            raise NotImplemented\n",
    "        \n",
    "        target = torch.stack((torch.cos(self.multi_nets[0](zsamples.view(-1,1)).squeeze()), torch.sin(self.multi_nets[0](zsamples.view(-1,1)).squeeze())))\n",
    "        model_input1 = torch.stack((torch.cos(self.multi_nets[1](zsamples.view(-1,1)).squeeze()), torch.sin(self.multi_nets[1](zsamples.view(-1,1)).squeeze())))\n",
    "        model_input2 = torch.stack((-torch.cos(self.multi_nets[1](zsamples.view(-1,1)).squeeze()), torch.sin(self.multi_nets[1](zsamples.view(-1,1)).squeeze())))\n",
    " \n",
    "        # Maybe MSE makes more sense here since we are just comparing functions in the first place\n",
    "        loss_out1 = torch.pow(F.mse_loss(model_input1, target), -self.p_value)\n",
    "        loss_out2 = torch.pow(F.mse_loss(model_input2, target), -self.p_value)\n",
    "        \n",
    "        return loss_out1 + loss_out2\n",
    "    \n",
    "    def complete_loss(self, zsamples, dual_active, logger=None):\n",
    "        \n",
    "        if not self.multiple_nets:\n",
    "            return self.loss_function(zsamples)\n",
    "        else:\n",
    "            if len(self.multi_nets)!=2:\n",
    "                raise NotImplemented\n",
    "            if dual_active:\n",
    "                repulsive_loss = self.dual_loss_ambiguity(zsamples)\n",
    "            else: \n",
    "                repulsive_loss = 0.0\n",
    "            \n",
    "            if logger is not None:\n",
    "                logger['metrics/repulsive_loss'].log(repulsive_loss)\n",
    "                \n",
    "            return self.multi_loss_function(zsamples, logger=logger) + self.lambda_repulsive * repulsive_loss\n",
    "    \n",
    "    def point_loss(self, zsamples):\n",
    "        \"\"\"\n",
    "        If we want to know the residuals at each given z point. This is useful for evaluation purposes\n",
    "        \"\"\"\n",
    "        target = torch.ones(zsamples.size(), device=self.device) if self.scaled_loss else self.mod_net(zsamples)*torch.sin(self.phi_net(zsamples.view(-1,1)).squeeze())\n",
    "        model_input = self.integral_approximator(zsamples)\n",
    "        \n",
    "        if self.scaled_loss:\n",
    "            model_input = model_input / (self.mod_net(zsamples)*torch.sin(self.phi_net(zsamples.view(-1,1)).squeeze()))\n",
    "        return torch.square(model_input - target)\n",
    "        \n",
    "    def train(self, neptune_run):\n",
    "        \"\"\"\n",
    "        Main training loop. \n",
    "        Each epoch is a single optimization step over a unique batch.\n",
    "        A batch is composed of a number of randomly selected z points between -1 and 1\n",
    "        \"\"\"\n",
    "        \n",
    "       \n",
    "        tq_iterator = tqdm(range(self.epochs), unit='epoch')\n",
    "        \n",
    "        for epoch_num in tq_iterator:\n",
    "            \n",
    "            dual_active = False if epoch_num < 0.1*self.epochs or epoch_num > 0.2*self.epochs else True\n",
    "            \n",
    "            # Set the network in train mode\n",
    "            if self.multiple_nets:\n",
    "                for net in self.multi_nets:\n",
    "                    net.train()\n",
    "            else:\n",
    "                self.phi_net.train()\n",
    "            \n",
    "            if self.device is not torch.device('cuda'):\n",
    "                zpoints = torch.rand(self.batch_size, requires_grad=False, generator=self.rand_gen, device='cpu').to(self.device)*2 - 1\n",
    "            else:\n",
    "                zpoints = torch.rand(self.batch_size, requires_grad=False, generator=self.rand_gen, device=self.device)*2 - 1\n",
    "            \n",
    "            # Zero the grads, get the loss and backpropagate\n",
    "            \n",
    "            if type(self.optimizer).__name__ == 'LBFGS':\n",
    "                \n",
    "                loss = self.complete_loss(zpoints, dual_active, logger=neptune_run)\n",
    "                \n",
    "                def closure():\n",
    "                    if torch.is_grad_enabled():\n",
    "                        self.optimizer.zero_grad()\n",
    "                    loss_comp = self.complete_loss(zpoints, dual_active, logger=neptune_run)\n",
    "                    if loss_comp.requires_grad:\n",
    "                        loss_comp.backward()\n",
    "                    return loss_comp\n",
    "                \n",
    "                if neptune_run is not None:\n",
    "                    neptune_run['metrics/learning_rate'].log(self.optimizer.param_groups[0][\"lr\"])\n",
    "                self.optimizer.step(closure)\n",
    "                   \n",
    "            else:\n",
    "                self.optimizer.zero_grad()\n",
    "                a =  torch.cuda.memory_allocated(device)\n",
    "                loss = self.complete_loss(zpoints, dual_active, logger=neptune_run)\n",
    "                b = torch.cuda.memory_allocated(device)\n",
    "                loss.backward()\n",
    "                if neptune_run is not None:\n",
    "                    neptune_run['metrics/learning_rate'].log(self.optimizer.param_groups[0][\"lr\"])\n",
    "                self.optimizer.step()\n",
    "                \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "            if neptune_run is not None:\n",
    "                neptune_run['metrics/train_loss'].log(loss)\n",
    "            \n",
    "            tq_iterator.set_description('Train Epoch: {} ; tLoss: {:.6f}'.format(epoch_num,loss.item()))\n",
    "\n",
    "def scheduler_rate(step, factor, size_param, warmup):\n",
    "    \"\"\"\n",
    "    For the Learning Rate scheduler we implement a warmup start, followed by a square root decay\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * size_param**(-0.5) * min(step**(-0.5), step*warmup ** (-1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2362cef4",
   "metadata": {},
   "source": [
    "##### Define the training and the evaluation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a25c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "def train_run(parameters, device, run_neptune=None, module_net=None, seed_num=42, optim_name='Adam'):\n",
    "    \"\"\"\n",
    "    Function to call for launching the training run\n",
    "    \"\"\"\n",
    "    # Fix for the generator : For reproducibility between cpu and MPS\n",
    "    random_gen_cpu = Generator(device='cpu')\n",
    "    random_gen_cpu.manual_seed(seed_num)\n",
    "    torch.manual_seed(seed_num)   \n",
    "    torch.cuda.manual_seed(seed_num)\n",
    "    \n",
    "    num_phi_nets = parameters['num_nets']\n",
    "    \n",
    "    # Define and register the networks\n",
    "    if num_phi_nets>1:\n",
    "        phinn = [PhiNet(parameters['layer_list'], parameters['activation'], parameters['fund_dom'], device=device, bound_phi=parameters['bound_phi'], final_layer=parameters['final_layer']) for i in range(num_phi_nets)]\n",
    "        phidisp = phinn[0]\n",
    "    else:\n",
    "        phinn = PhiNet(parameters['layer_list'], parameters['activation'], parameters['fund_dom'], device=device, bound_phi=parameters['bound_phi'], final_layer=parameters['final_layer'])\n",
    "        phidisp = phinn\n",
    "    \n",
    "    if module_net is None:\n",
    "        modnn = ModuleNetManual(scan_param=parameters['scan_param'])\n",
    "    else:\n",
    "        modnn = module_net\n",
    "    \n",
    "    # Define and register the optimizer and scheduler\n",
    "    if optim_name == 'Adam':\n",
    "        if num_phi_nets>1:\n",
    "            optimizer = torch.optim.Adam(chain.from_iterable([phinet.parameters() for phinet in phinn]), lr=parameters['learning_rate'], betas=(parameters['beta1'], parameters['beta2']))\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(phinn.parameters(), lr=parameters['learning_rate'], betas=(parameters['beta1'], parameters['beta2']))\n",
    "    elif optim_name == 'LBFGS':\n",
    "        if num_phi_nets>1:\n",
    "            optimizer = torch.optim.LBFGS(chain.from_iterable([phinet.parameters() for phinet in phinn]), lr=parameters['learning_rate'], max_iter=4)\n",
    "        else:\n",
    "            optimizer = torch.optim.LBFGS(phinn.parameters(), lr=parameters['learning_rate'], max_iter=4)\n",
    "    \n",
    "    \n",
    "    if parameters['lr_scheduler']:\n",
    "        #lr_scheduler = LambdaLR(optimizer=optimizer, lr_lambda=lambda step: scheduler_rate(step, 1/parameters['learning_rate'], 2048, 750))\n",
    "        lr_scheduler = ExponentialLR(optimizer=optimizer, gamma=0.995)\n",
    "    else:\n",
    "        lr_scheduler = None\n",
    "    \n",
    "    # Create the simulation, train it and return the trained result\n",
    "    netsolver = PhaseNetSolver(parameters, phinn, modnn, optimizer, random_gen_cpu, lr_scheduler, device=device)\n",
    "    netsolver.train(run_neptune)\n",
    "\n",
    "    return netsolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520a0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_run_loss(trained_solver, device, steps_eval=100):\n",
    "    \"\"\"\n",
    "    At evaluation we verify if the integral equation is satisfied. \n",
    "    For this we take a larger sample of z points\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure we are in evaluation mode\n",
    "    trained_solver.phi_net.eval()\n",
    "    with torch.no_grad(): \n",
    "        \n",
    "        # Create the evaluation points and compute the phase at those points\n",
    "        zpointstest = torch.linspace(-1,1,steps=steps_eval, device=device)\n",
    "        \n",
    "        # Also log the final loss values. Including the loss at individual z values       \n",
    "        eval_loss_base = trained_solver.multi_loss_function(zpointstest)\n",
    "        \n",
    "    return eval_loss_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b08f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleInterpCrichton(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for parametrizing the given differential cross section\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda_param=1):\n",
    "        super(ModuleInterpCrichton, self).__init__()\n",
    "        self.lambda_param = lambda_param\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Network forward pass is given by a specific function. Can iterate on the overall scale if we want to\n",
    "        \"\"\"\n",
    "        #return torch.abs(np.exp(1j *659 * math.pi /1200) * np.cos(59*math.pi /1200) + 5*np.exp(1j*self.lambda_param*math.pi /9)*(3*torch.square(x)-1) * np.sin(self.lambda_param*math.pi /9)/2 - 3*np.exp(-59*1j*self.lambda_param*math.pi /400)* x * np.sin(59 * self.lambda_param*math.pi /400))\n",
    "        return torch.sqrt((16*np.cos((59*math.pi)/1200.)**4 + 4*np.sin((59*math.pi)/600.)**2 + 100*(1 - 3*torch.square(x))**2*np.sin((math.pi*self.lambda_param)/9.)**4 + 96*x*np.cos((59*math.pi)/1200.)**2*np.sin((59*math.pi*self.lambda_param)/400.)**2 + 144*torch.square(x)*np.sin((59*math.pi*self.lambda_param)/400.)**4 + 80*(-1 + 3*torch.square(x))*np.sin((math.pi*self.lambda_param)/9.)**2*(np.cos((59*math.pi)/1200.)**2 + 3*x*np.sin((59*math.pi*self.lambda_param)/400.)**2) + 20*np.sin((59*math.pi)/600.)*np.sin((2*math.pi*self.lambda_param)/9.) - 60*torch.square(x)*np.sin((59*math.pi)/600.)*np.sin((2*math.pi*self.lambda_param)/9.) + 25*np.sin((2*math.pi*self.lambda_param)/9.)**2 - 150*torch.square(x)*np.sin((2*math.pi*self.lambda_param)/9.)**2 + 225*torch.pow(x,4)*np.sin((2*math.pi*self.lambda_param)/9.)**2 + 24*x*np.sin((59*math.pi)/600.)*np.sin((59*math.pi*self.lambda_param)/200.) + 60*x*np.sin((2*math.pi*self.lambda_param)/9.)*np.sin((59*math.pi*self.lambda_param)/200.) - 180*torch.pow(x,3)*np.sin((2*math.pi*self.lambda_param)/9.)*np.sin((59*math.pi*self.lambda_param)/200.) + 36*torch.square(x)*np.sin((59*math.pi*self.lambda_param)/200.)**2)/16.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad164b9",
   "metadata": {},
   "source": [
    "##### Define the Optuna Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9ae0e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    params = {'batch_size': trial.suggest_int(\"batch_size\",4, 128, 4),\n",
    "             'epochs_num': 2000,\n",
    "             'learning_rate': trial.suggest_float(\"learning_rate\", 0.0001, 0.01, log=True),\n",
    "             'lr_scheduler': False, \n",
    "             'beta1' : trial.suggest_float(\"beta1\", 0.8, 0.95, step=0.025),\n",
    "             'beta2' : trial.suggest_float(\"beta2\", 0.9, 0.9999, log=True),\n",
    "             'integral_points': trial.suggest_int(\"integral_points\", 15, 50, 5),\n",
    "             'method_int': 'trapz',\n",
    "             'activation': trial.suggest_categorical(\"activation\", ['ReLU', 'Tanh']),\n",
    "             'fund_dom': False,\n",
    "             'n_layers': trial.suggest_int(\"n_layers\", 1, 5),\n",
    "             'n_neurons': trial.suggest_int(\"n_neurons\", 4, 136, 12),\n",
    "             'final_layer': trial.suggest_categorical(\"final_layer\", ['Sigmoid', 'Tanh']),\n",
    "             'loss': 'MSE',\n",
    "             'scaled_loss': False,\n",
    "             'scan_param': 1,\n",
    "             'bound_phi': math.pi,\n",
    "             'num_nets': 2,\n",
    "             'p_value': trial.suggest_float(\"p_value\", 1, 3, step=0.25),\n",
    "             'lambda_repulsive': trial.suggest_float(\"lambda_repulsive\", 0.1, 5, log=True)}\n",
    "    \n",
    "    params['layer_list'] = params['n_layers'] * [params['n_neurons']]\n",
    "    \n",
    "    crichtonnet = ModuleInterpCrichton(lambda_param=params['scan_param'])\n",
    "    netsolver = train_run(params, device, module_net=crichtonnet, seed_num=42, optim_name='Adam')\n",
    "    \n",
    "    eval_loss = eval_run_loss(netsolver, device)\n",
    "    \n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8fc67a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/zulap/2to2scattering/e/TOS-1015\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-31 19:27:37,611]\u001b[0m A new study created in memory with name: no-name-3e70497c-680d-423d-84b3-8dd45e898669\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000202: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [06:22<00:00,  5.23epoch/s]\n",
      "\u001b[32m[I 2023-01-31 19:34:00,148]\u001b[0m Trial 0 finished with value: 0.00013260490959510207 and parameters: {'batch_size': 120, 'learning_rate': 0.005769743573038201, 'beta1': 0.925, 'beta2': 0.9208945856594608, 'integral_points': 15, 'activation': 'ReLU', 'n_layers': 3, 'n_neurons': 52, 'final_layer': 'Sigmoid', 'p_value': 2.25, 'lambda_repulsive': 0.5633962588012414}. Best is trial 0 with value: 0.00013260490959510207.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000037: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [13:54<00:00,  2.40epoch/s]\n",
      "\u001b[32m[I 2023-01-31 19:47:55,371]\u001b[0m Trial 1 finished with value: 1.3852983101969585e-05 and parameters: {'batch_size': 48, 'learning_rate': 0.00028381072162609005, 'beta1': 0.8, 'beta2': 0.9637450056633334, 'integral_points': 45, 'activation': 'ReLU', 'n_layers': 5, 'n_neurons': 64, 'final_layer': 'Tanh', 'p_value': 1.5, 'lambda_repulsive': 0.14220988243818067}. Best is trial 1 with value: 1.3852983101969585e-05.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000227: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:53<00:00, 11.52epoch/s]\n",
      "\u001b[32m[I 2023-01-31 19:50:49,059]\u001b[0m Trial 2 finished with value: 5.563151717069559e-05 and parameters: {'batch_size': 52, 'learning_rate': 0.00034882655674671145, 'beta1': 0.925, 'beta2': 0.9392840478850386, 'integral_points': 20, 'activation': 'Tanh', 'n_layers': 2, 'n_neurons': 40, 'final_layer': 'Tanh', 'p_value': 1.5, 'lambda_repulsive': 0.5553039368292961}. Best is trial 1 with value: 1.3852983101969585e-05.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.001481: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:57<00:00, 11.29epoch/s]\n",
      "\u001b[32m[I 2023-01-31 19:53:46,294]\u001b[0m Trial 3 finished with value: 0.00048808797146193683 and parameters: {'batch_size': 4, 'learning_rate': 0.0013660645830870799, 'beta1': 0.95, 'beta2': 0.9166556396495487, 'integral_points': 45, 'activation': 'Tanh', 'n_layers': 4, 'n_neurons': 52, 'final_layer': 'Tanh', 'p_value': 2.75, 'lambda_repulsive': 3.7087632255746774}. Best is trial 1 with value: 1.3852983101969585e-05.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000933: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [06:33<00:00,  5.08epoch/s]\n",
      "\u001b[32m[I 2023-01-31 20:00:19,935]\u001b[0m Trial 4 finished with value: 0.0010206850711256266 and parameters: {'batch_size': 44, 'learning_rate': 0.002355660953055491, 'beta1': 0.875, 'beta2': 0.913912012331972, 'integral_points': 35, 'activation': 'Tanh', 'n_layers': 4, 'n_neurons': 76, 'final_layer': 'Sigmoid', 'p_value': 1.0, 'lambda_repulsive': 1.0145524591240511}. Best is trial 1 with value: 1.3852983101969585e-05.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.002106: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [18:05<00:00,  1.84epoch/s]\n",
      "\u001b[32m[I 2023-01-31 20:18:26,068]\u001b[0m Trial 5 finished with value: 0.0010358128929510713 and parameters: {'batch_size': 20, 'learning_rate': 0.0001880718751652073, 'beta1': 0.925, 'beta2': 0.9044024240970104, 'integral_points': 50, 'activation': 'ReLU', 'n_layers': 5, 'n_neurons': 40, 'final_layer': 'Sigmoid', 'p_value': 3.0, 'lambda_repulsive': 0.1958023764797159}. Best is trial 1 with value: 1.3852983101969585e-05.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000019: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [21:07<00:00,  1.58epoch/s]\n",
      "\u001b[32m[I 2023-01-31 20:39:33,646]\u001b[0m Trial 6 finished with value: 2.842416733983555e-06 and parameters: {'batch_size': 108, 'learning_rate': 0.00042860729124663023, 'beta1': 0.875, 'beta2': 0.9767114348637456, 'integral_points': 40, 'activation': 'ReLU', 'n_layers': 4, 'n_neurons': 76, 'final_layer': 'Sigmoid', 'p_value': 1.5, 'lambda_repulsive': 0.28288768826943067}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000059: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [09:04<00:00,  3.67epoch/s]\n",
      "\u001b[32m[I 2023-01-31 20:48:38,116]\u001b[0m Trial 7 finished with value: 5.3995187045075e-05 and parameters: {'batch_size': 100, 'learning_rate': 0.00041531106547571103, 'beta1': 0.8, 'beta2': 0.9466727197183099, 'integral_points': 20, 'activation': 'ReLU', 'n_layers': 4, 'n_neurons': 136, 'final_layer': 'Sigmoid', 'p_value': 1.25, 'lambda_repulsive': 1.8371596168016537}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000180: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [19:01<00:00,  1.75epoch/s]\n",
      "\u001b[32m[I 2023-01-31 21:07:39,798]\u001b[0m Trial 8 finished with value: 5.168448842596263e-05 and parameters: {'batch_size': 124, 'learning_rate': 0.00041785895033262673, 'beta1': 0.8, 'beta2': 0.9269718958335067, 'integral_points': 45, 'activation': 'ReLU', 'n_layers': 3, 'n_neurons': 28, 'final_layer': 'Sigmoid', 'p_value': 1.25, 'lambda_repulsive': 3.226213325280422}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.034177: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:02<00:00, 10.96epoch/s]\n",
      "\u001b[32m[I 2023-01-31 21:10:42,349]\u001b[0m Trial 9 finished with value: 0.01453201100230217 and parameters: {'batch_size': 92, 'learning_rate': 0.0003487905596101115, 'beta1': 0.8, 'beta2': 0.936570655799417, 'integral_points': 20, 'activation': 'Tanh', 'n_layers': 2, 'n_neurons': 28, 'final_layer': 'Tanh', 'p_value': 1.75, 'lambda_repulsive': 1.3551969418173007}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.188327: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:25<00:00,  9.72epoch/s]\n",
      "\u001b[32m[I 2023-01-31 21:14:08,398]\u001b[0m Trial 10 finished with value: 0.1569608896970749 and parameters: {'batch_size': 84, 'learning_rate': 0.00010051842159938677, 'beta1': 0.8500000000000001, 'beta2': 0.9976991744316799, 'integral_points': 35, 'activation': 'ReLU', 'n_layers': 1, 'n_neurons': 100, 'final_layer': 'Sigmoid', 'p_value': 2.25, 'lambda_repulsive': 0.3063716382120325}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.001575: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [16:05<00:00,  2.07epoch/s]\n",
      "\u001b[32m[I 2023-01-31 21:30:14,809]\u001b[0m Trial 11 finished with value: 0.0004801713803317398 and parameters: {'batch_size': 60, 'learning_rate': 0.0008046972033676501, 'beta1': 0.8500000000000001, 'beta2': 0.9736158141779161, 'integral_points': 40, 'activation': 'ReLU', 'n_layers': 5, 'n_neurons': 88, 'final_layer': 'Tanh', 'p_value': 1.75, 'lambda_repulsive': 0.10694961037158639}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000017: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:31<00:00,  7.38epoch/s]\n",
      "\u001b[32m[I 2023-01-31 21:34:47,077]\u001b[0m Trial 12 finished with value: 9.05265005712863e-06 and parameters: {'batch_size': 32, 'learning_rate': 0.00016848699289546168, 'beta1': 0.8500000000000001, 'beta2': 0.964228449608702, 'integral_points': 30, 'activation': 'ReLU', 'n_layers': 5, 'n_neurons': 112, 'final_layer': 'Tanh', 'p_value': 2.0, 'lambda_repulsive': 0.10216600897181488}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.004798: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:14<00:00,  7.85epoch/s]\n",
      "\u001b[32m[I 2023-01-31 21:39:02,195]\u001b[0m Trial 13 finished with value: 2.8087462851544842e-05 and parameters: {'batch_size': 28, 'learning_rate': 0.00012016969178794626, 'beta1': 0.8500000000000001, 'beta2': 0.9636402555290501, 'integral_points': 30, 'activation': 'ReLU', 'n_layers': 4, 'n_neurons': 124, 'final_layer': 'Tanh', 'p_value': 2.25, 'lambda_repulsive': 0.25299321933435925}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1999 ; tLoss: 0.831416: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:36<00:00,  9.22epoch/s]\n",
      "\u001b[32m[I 2023-01-31 21:42:39,121]\u001b[0m Trial 14 finished with value: 0.49173545837402344 and parameters: {'batch_size': 76, 'learning_rate': 0.00018035411231854664, 'beta1': 0.875, 'beta2': 0.9799223150373702, 'integral_points': 30, 'activation': 'ReLU', 'n_layers': 5, 'n_neurons': 4, 'final_layer': 'Sigmoid', 'p_value': 2.0, 'lambda_repulsive': 0.1006878594487525}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000442: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [09:48<00:00,  3.40epoch/s]\n",
      "\u001b[32m[I 2023-01-31 21:52:28,028]\u001b[0m Trial 15 finished with value: 0.0001208117391797714 and parameters: {'batch_size': 108, 'learning_rate': 0.0006716892390874636, 'beta1': 0.9, 'beta2': 0.9563303845240164, 'integral_points': 25, 'activation': 'ReLU', 'n_layers': 4, 'n_neurons': 100, 'final_layer': 'Tanh', 'p_value': 2.0, 'lambda_repulsive': 0.32828433017095776}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.002788: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [32:10<00:00,  1.04epoch/s]\n",
      "\u001b[32m[I 2023-01-31 22:24:39,361]\u001b[0m Trial 16 finished with value: 0.0024913346860557795 and parameters: {'batch_size': 68, 'learning_rate': 0.00018303482017764024, 'beta1': 0.8250000000000001, 'beta2': 0.9843804624982556, 'integral_points': 40, 'activation': 'ReLU', 'n_layers': 5, 'n_neurons': 112, 'final_layer': 'Sigmoid', 'p_value': 2.5, 'lambda_repulsive': 0.1845096954872525}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1397 ; tLoss: 0.008209:  70%|██████████████████████████████████████████████████████████████▉                           | 1398/2000 [03:33<01:22,  7.33epoch/s]Exception in thread Exception in thread NeptuneWebhooks:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_socket.py\", line 108, in recv\n",
      "Exception in thread NeptuneWebhooks:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_socket.py\", line 108, in recv\n",
      "NeptuneWebhooks:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_socket.py\", line 108, in recv\n",
      "    bytes_ = _recv()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_socket.py\", line 87, in _recv\n",
      "    bytes_ = _recv()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_socket.py\", line 87, in _recv\n",
      "    bytes_ = _recv()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_socket.py\", line 87, in _recv\n",
      "    return sock.recv(bufsize)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/ssl.py\", line 1227, in recv\n",
      "    return sock.recv(bufsize)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/ssl.py\", line 1227, in recv\n",
      "    return sock.recv(bufsize)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/ssl.py\", line 1227, in recv\n",
      "    return self.read(buflen)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/ssl.py\", line 1102, in read\n",
      "    return self.read(buflen)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/ssl.py\", line 1102, in read\n",
      "    return self.read(buflen)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/ssl.py\", line 1102, in read\n",
      "    return self._sslobj.read(len)\n",
      "TimeoutError: [Errno 60] Operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    return self._sslobj.read(len)\n",
      "TimeoutError: [Errno 60] Operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    return self._sslobj.read(len)\n",
      "TimeoutError: [Errno 60] Operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/threading.py\", line 980, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/neptune/new/internal/threading/daemon.py\", line 51, in run\n",
      "    self.run()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/neptune/new/internal/threading/daemon.py\", line 51, in run\n",
      "    self.run()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/neptune/new/internal/threading/daemon.py\", line 51, in run\n",
      "    self.work()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/neptune/new/internal/websockets/websocket_signals_background_job.py\", line 77, in work\n",
      "    self.work()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/neptune/new/internal/websockets/websocket_signals_background_job.py\", line 77, in work\n",
      "    self.work()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/neptune/new/internal/websockets/websocket_signals_background_job.py\", line 77, in work\n",
      "    raw_message = self._ws_client.recv()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/neptune/common/websockets/reconnecting_websocket.py\", line 51, in recv\n",
      "    raw_message = self._ws_client.recv()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/neptune/common/websockets/reconnecting_websocket.py\", line 51, in recv\n",
      "    raw_message = self._ws_client.recv()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/neptune/common/websockets/reconnecting_websocket.py\", line 51, in recv\n",
      "    data = self.client.recv()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/neptune/common/websockets/websocket_client_adapter.py\", line 63, in recv\n",
      "    data = self.client.recv()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/neptune/common/websockets/websocket_client_adapter.py\", line 63, in recv\n",
      "    data = self.client.recv()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/neptune/common/websockets/websocket_client_adapter.py\", line 63, in recv\n",
      "    opcode, data = self._ws_client.recv_data()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_core.py\", line 385, in recv_data\n",
      "    opcode, data = self._ws_client.recv_data()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_core.py\", line 385, in recv_data\n",
      "    opcode, data = self._ws_client.recv_data()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_core.py\", line 385, in recv_data\n",
      "    opcode, frame = self.recv_data_frame(control_frame)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_core.py\", line 406, in recv_data_frame\n",
      "    opcode, frame = self.recv_data_frame(control_frame)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_core.py\", line 406, in recv_data_frame\n",
      "    opcode, frame = self.recv_data_frame(control_frame)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_core.py\", line 406, in recv_data_frame\n",
      "    frame = self.recv_frame()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_core.py\", line 445, in recv_frame\n",
      "    frame = self.recv_frame()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_core.py\", line 445, in recv_frame\n",
      "    return self.frame_buffer.recv_frame()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_abnf.py\", line 338, in recv_frame\n",
      "    frame = self.recv_frame()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_core.py\", line 445, in recv_frame\n",
      "    self.recv_header()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_abnf.py\", line 294, in recv_header\n",
      "    return self.frame_buffer.recv_frame()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_abnf.py\", line 338, in recv_frame\n",
      "    header = self.recv_strict(2)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_abnf.py\", line 373, in recv_strict\n",
      "    return self.frame_buffer.recv_frame()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_abnf.py\", line 338, in recv_frame\n",
      "    self.recv_header()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_abnf.py\", line 294, in recv_header\n",
      "    bytes_ = self.recv(min(16384, shortage))\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_core.py\", line 529, in _recv\n",
      "    header = self.recv_strict(2)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_abnf.py\", line 373, in recv_strict\n",
      "    self.recv_header()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_abnf.py\", line 294, in recv_header\n",
      "    bytes_ = self.recv(min(16384, shortage))\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_core.py\", line 529, in _recv\n",
      "    header = self.recv_strict(2)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_abnf.py\", line 373, in recv_strict\n",
      "    return recv(self.sock, bufsize)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_socket.py\", line 110, in recv\n",
      "    return recv(self.sock, bufsize)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_socket.py\", line 110, in recv\n",
      "    bytes_ = self.recv(min(16384, shortage))\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_core.py\", line 529, in _recv\n",
      "    raise WebSocketTimeoutException(\"Connection timed out\")\n",
      "websocket._exceptions.WebSocketTimeoutException: Connection timed out\n",
      "    raise WebSocketTimeoutException(\"Connection timed out\")\n",
      "websocket._exceptions.WebSocketTimeoutException: Connection timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    return recv(self.sock, bufsize)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/websocket/_socket.py\", line 110, in recv\n",
      "    raise WebSocketTimeoutException(\"Connection timed out\")\n",
      "websocket._exceptions.WebSocketTimeoutException: Connection timed out\n",
      "Train Epoch: 1999 ; tLoss: 0.005926: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:55<00:00,  6.76epoch/s]\n",
      "\u001b[32m[I 2023-01-31 22:29:35,395]\u001b[0m Trial 17 finished with value: 6.312422556220554e-06 and parameters: {'batch_size': 32, 'learning_rate': 0.0005867881865135276, 'beta1': 0.9, 'beta2': 0.9532579374311311, 'integral_points': 40, 'activation': 'ReLU', 'n_layers': 3, 'n_neurons': 76, 'final_layer': 'Tanh', 'p_value': 1.75, 'lambda_repulsive': 0.40461815599115825}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000191: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [02:35<00:00, 12.86epoch/s]\n",
      "\u001b[32m[I 2023-01-31 22:32:11,128]\u001b[0m Trial 18 finished with value: 0.0009214322199113667 and parameters: {'batch_size': 4, 'learning_rate': 0.0014734863330962186, 'beta1': 0.9, 'beta2': 0.9511731936093599, 'integral_points': 40, 'activation': 'Tanh', 'n_layers': 2, 'n_neurons': 76, 'final_layer': 'Sigmoid', 'p_value': 1.0, 'lambda_repulsive': 0.43037478151102954}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.003254: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [24:48<00:00,  1.34epoch/s]\n",
      "\u001b[32m[I 2023-01-31 22:56:59,660]\u001b[0m Trial 19 finished with value: 0.0021788161247968674 and parameters: {'batch_size': 112, 'learning_rate': 0.0007069218427700034, 'beta1': 0.9, 'beta2': 0.9991841558557252, 'integral_points': 50, 'activation': 'ReLU', 'n_layers': 3, 'n_neurons': 88, 'final_layer': 'Tanh', 'p_value': 1.5, 'lambda_repulsive': 0.6735932349395887}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000124: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:04<00:00, 10.86epoch/s]\n",
      "\u001b[32m[I 2023-01-31 23:00:04,028]\u001b[0m Trial 20 finished with value: 3.2545332942390814e-05 and parameters: {'batch_size': 20, 'learning_rate': 0.0006336240905274445, 'beta1': 0.875, 'beta2': 0.9713823794332729, 'integral_points': 35, 'activation': 'ReLU', 'n_layers': 3, 'n_neurons': 64, 'final_layer': 'Sigmoid', 'p_value': 1.75, 'lambda_repulsive': 0.3504822868743734}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000487: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:28<00:00,  7.44epoch/s]\n",
      "\u001b[32m[I 2023-01-31 23:04:32,906]\u001b[0m Trial 21 finished with value: 2.2157462808536366e-05 and parameters: {'batch_size': 36, 'learning_rate': 0.00024752666037577574, 'beta1': 0.875, 'beta2': 0.9578477374382315, 'integral_points': 30, 'activation': 'ReLU', 'n_layers': 4, 'n_neurons': 112, 'final_layer': 'Tanh', 'p_value': 1.75, 'lambda_repulsive': 0.20710927872297766}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000947: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [09:39<00:00,  3.45epoch/s]\n",
      "\u001b[32m[I 2023-01-31 23:14:12,368]\u001b[0m Trial 22 finished with value: 0.00022864654602017254 and parameters: {'batch_size': 32, 'learning_rate': 0.0005428375338524385, 'beta1': 0.8250000000000001, 'beta2': 0.96650847219328, 'integral_points': 40, 'activation': 'ReLU', 'n_layers': 4, 'n_neurons': 136, 'final_layer': 'Tanh', 'p_value': 2.0, 'lambda_repulsive': 0.1438972535243258}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.002414: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:29<00:00,  7.41epoch/s]\n",
      "\u001b[32m[I 2023-01-31 23:18:42,264]\u001b[0m Trial 23 finished with value: 0.002063277643173933 and parameters: {'batch_size': 64, 'learning_rate': 0.000485008810517465, 'beta1': 0.8500000000000001, 'beta2': 0.9823998650607628, 'integral_points': 25, 'activation': 'ReLU', 'n_layers': 3, 'n_neurons': 88, 'final_layer': 'Tanh', 'p_value': 1.25, 'lambda_repulsive': 0.4385865462286963}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000085: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:23<00:00,  7.60epoch/s]\n",
      "\u001b[32m[I 2023-01-31 23:23:05,541]\u001b[0m Trial 24 finished with value: 0.0001375486608594656 and parameters: {'batch_size': 20, 'learning_rate': 0.0002919071593412886, 'beta1': 0.9, 'beta2': 0.9484526991931608, 'integral_points': 35, 'activation': 'ReLU', 'n_layers': 5, 'n_neurons': 112, 'final_layer': 'Tanh', 'p_value': 1.5, 'lambda_repulsive': 0.28491267123454433}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000748: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:04<00:00, 10.83epoch/s]\n",
      "\u001b[32m[I 2023-01-31 23:26:10,360]\u001b[0m Trial 25 finished with value: 0.0001002510980470106 and parameters: {'batch_size': 40, 'learning_rate': 0.000958348374137119, 'beta1': 0.8250000000000001, 'beta2': 0.9566155813428089, 'integral_points': 45, 'activation': 'ReLU', 'n_layers': 1, 'n_neurons': 100, 'final_layer': 'Tanh', 'p_value': 2.5, 'lambda_repulsive': 0.24292570162866817}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.006975: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:59<00:00,  8.33epoch/s]\n",
      "\u001b[32m[I 2023-01-31 23:30:10,451]\u001b[0m Trial 26 finished with value: 0.00011743489449145272 and parameters: {'batch_size': 56, 'learning_rate': 0.000135008027797185, 'beta1': 0.875, 'beta2': 0.9735653952914313, 'integral_points': 25, 'activation': 'ReLU', 'n_layers': 3, 'n_neurons': 76, 'final_layer': 'Tanh', 'p_value': 1.75, 'lambda_repulsive': 0.15036967736409773}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000023: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [08:25<00:00,  3.96epoch/s]\n",
      "\u001b[32m[I 2023-01-31 23:38:36,287]\u001b[0m Trial 27 finished with value: 0.00010582010872894898 and parameters: {'batch_size': 76, 'learning_rate': 0.00023846774740830123, 'beta1': 0.95, 'beta2': 0.9421167320763416, 'integral_points': 40, 'activation': 'Tanh', 'n_layers': 2, 'n_neurons': 124, 'final_layer': 'Tanh', 'p_value': 2.0, 'lambda_repulsive': 0.42082106764526817}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000404: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:19<00:00, 10.03epoch/s]\n",
      "\u001b[32m[I 2023-01-31 23:41:55,867]\u001b[0m Trial 28 finished with value: 7.649907638551667e-05 and parameters: {'batch_size': 16, 'learning_rate': 0.0004939874262585058, 'beta1': 0.9, 'beta2': 0.9883555723692741, 'integral_points': 30, 'activation': 'ReLU', 'n_layers': 4, 'n_neurons': 64, 'final_layer': 'Sigmoid', 'p_value': 1.25, 'lambda_repulsive': 0.8767655306505705}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000167: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [09:47<00:00,  3.40epoch/s]\n",
      "\u001b[32m[I 2023-01-31 23:51:43,619]\u001b[0m Trial 29 finished with value: 7.312358502531424e-05 and parameters: {'batch_size': 116, 'learning_rate': 0.0056312623326252404, 'beta1': 0.925, 'beta2': 0.9670530715107806, 'integral_points': 15, 'activation': 'ReLU', 'n_layers': 5, 'n_neurons': 52, 'final_layer': 'Sigmoid', 'p_value': 2.0, 'lambda_repulsive': 0.5551037072463875}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 2.194682: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [04:16<00:00,  7.81epoch/s]\n",
      "\u001b[32m[I 2023-01-31 23:55:59,961]\u001b[0m Trial 30 finished with value: 2.190613031387329 and parameters: {'batch_size': 128, 'learning_rate': 0.0003553058357452818, 'beta1': 0.8500000000000001, 'beta2': 0.9777278494246594, 'integral_points': 35, 'activation': 'ReLU', 'n_layers': 3, 'n_neurons': 4, 'final_layer': 'Tanh', 'p_value': 2.5, 'lambda_repulsive': 0.2352533921584309}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000016: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [14:21<00:00,  2.32epoch/s]\n",
      "\u001b[32m[I 2023-02-01 00:10:21,726]\u001b[0m Trial 31 finished with value: 4.907858055958059e-06 and parameters: {'batch_size': 48, 'learning_rate': 0.0002550612249672448, 'beta1': 0.8250000000000001, 'beta2': 0.9630855341878617, 'integral_points': 45, 'activation': 'ReLU', 'n_layers': 5, 'n_neurons': 64, 'final_layer': 'Tanh', 'p_value': 1.5, 'lambda_repulsive': 0.13871333259333404}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000036: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [15:40<00:00,  2.13epoch/s]\n",
      "\u001b[32m[I 2023-02-01 00:26:02,595]\u001b[0m Trial 32 finished with value: 6.195413334353361e-06 and parameters: {'batch_size': 44, 'learning_rate': 0.00023243095749992876, 'beta1': 0.8250000000000001, 'beta2': 0.9619496228871525, 'integral_points': 50, 'activation': 'ReLU', 'n_layers': 5, 'n_neurons': 64, 'final_layer': 'Tanh', 'p_value': 1.5, 'lambda_repulsive': 0.11472153204626914}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000019: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [16:39<00:00,  2.00epoch/s]\n",
      "\u001b[32m[I 2023-02-01 00:42:42,441]\u001b[0m Trial 33 finished with value: 1.1533586985024158e-05 and parameters: {'batch_size': 48, 'learning_rate': 0.0003012272888850656, 'beta1': 0.8250000000000001, 'beta2': 0.959382424800441, 'integral_points': 50, 'activation': 'ReLU', 'n_layers': 5, 'n_neurons': 52, 'final_layer': 'Tanh', 'p_value': 1.5, 'lambda_repulsive': 0.17289702330256446}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.007722: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [12:10<00:00,  2.74epoch/s]\n",
      "\u001b[32m[I 2023-02-01 00:54:52,668]\u001b[0m Trial 34 finished with value: 1.2825140402128454e-05 and parameters: {'batch_size': 52, 'learning_rate': 0.000236122896614417, 'beta1': 0.8250000000000001, 'beta2': 0.9537826683124645, 'integral_points': 45, 'activation': 'ReLU', 'n_layers': 4, 'n_neurons': 64, 'final_layer': 'Tanh', 'p_value': 1.5, 'lambda_repulsive': 0.14230933767168338}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.001608: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [12:40<00:00,  2.63epoch/s]\n",
      "\u001b[32m[I 2023-02-01 01:07:33,098]\u001b[0m Trial 35 finished with value: 0.0011085261357948184 and parameters: {'batch_size': 44, 'learning_rate': 0.00038763755202113326, 'beta1': 0.8, 'beta2': 0.9620231369197295, 'integral_points': 45, 'activation': 'Tanh', 'n_layers': 5, 'n_neurons': 40, 'final_layer': 'Tanh', 'p_value': 1.5, 'lambda_repulsive': 0.11879623821958273}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.001649: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [22:13<00:00,  1.50epoch/s]\n",
      "\u001b[32m[I 2023-02-01 01:29:46,857]\u001b[0m Trial 36 finished with value: 8.915993021219037e-06 and parameters: {'batch_size': 72, 'learning_rate': 0.0005413316432457434, 'beta1': 0.925, 'beta2': 0.9699942637654332, 'integral_points': 50, 'activation': 'ReLU', 'n_layers': 4, 'n_neurons': 76, 'final_layer': 'Tanh', 'p_value': 1.25, 'lambda_repulsive': 0.17144369461614797}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000062: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [15:25<00:00,  2.16epoch/s]\n",
      "\u001b[32m[I 2023-02-01 01:45:12,894]\u001b[0m Trial 37 finished with value: 3.3769250876503065e-05 and parameters: {'batch_size': 52, 'learning_rate': 0.00029183370011875553, 'beta1': 0.95, 'beta2': 0.9611094058873672, 'integral_points': 45, 'activation': 'Tanh', 'n_layers': 5, 'n_neurons': 52, 'final_layer': 'Tanh', 'p_value': 1.0, 'lambda_repulsive': 0.21496664209848435}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.007782: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [08:47<00:00,  3.79epoch/s]\n",
      "\u001b[32m[I 2023-02-01 01:54:00,898]\u001b[0m Trial 38 finished with value: 2.747716825979296e-05 and parameters: {'batch_size': 28, 'learning_rate': 0.00020847463357293665, 'beta1': 0.8250000000000001, 'beta2': 0.9526840698892168, 'integral_points': 50, 'activation': 'ReLU', 'n_layers': 4, 'n_neurons': 76, 'final_layer': 'Sigmoid', 'p_value': 1.5, 'lambda_repulsive': 0.1300838737377921}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.015198: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [03:39<00:00,  9.12epoch/s]\n",
      "\u001b[32m[I 2023-02-01 01:57:40,341]\u001b[0m Trial 39 finished with value: 0.00024210016999859363 and parameters: {'batch_size': 12, 'learning_rate': 0.00014485889258254242, 'beta1': 0.8, 'beta2': 0.9472537275410992, 'integral_points': 45, 'activation': 'ReLU', 'n_layers': 4, 'n_neurons': 40, 'final_layer': 'Tanh', 'p_value': 1.25, 'lambda_repulsive': 0.12730481751299716}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000073: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [23:31<00:00,  1.42epoch/s]\n",
      "\u001b[32m[I 2023-02-01 02:21:11,974]\u001b[0m Trial 40 finished with value: 1.93809719348792e-05 and parameters: {'batch_size': 100, 'learning_rate': 0.00046353441672691994, 'beta1': 0.875, 'beta2': 0.9683804234137716, 'integral_points': 40, 'activation': 'Tanh', 'n_layers': 5, 'n_neurons': 64, 'final_layer': 'Sigmoid', 'p_value': 1.75, 'lambda_repulsive': 0.1643333262293231}. Best is trial 6 with value: 2.842416733983555e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000644: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [16:07<00:00,  2.07epoch/s]\n",
      "\u001b[32m[I 2023-02-01 02:37:19,919]\u001b[0m Trial 41 finished with value: 2.067258947135997e-06 and parameters: {'batch_size': 68, 'learning_rate': 0.0005693513536793147, 'beta1': 0.925, 'beta2': 0.9701510375304025, 'integral_points': 50, 'activation': 'ReLU', 'n_layers': 3, 'n_neurons': 76, 'final_layer': 'Tanh', 'p_value': 1.25, 'lambda_repulsive': 0.18319816728189814}. Best is trial 41 with value: 2.067258947135997e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000812: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [19:34<00:00,  1.70epoch/s]\n",
      "\u001b[32m[I 2023-02-01 02:56:55,102]\u001b[0m Trial 42 finished with value: 0.001865703146904707 and parameters: {'batch_size': 84, 'learning_rate': 0.0003597138994965011, 'beta1': 0.925, 'beta2': 0.9730698655715937, 'integral_points': 50, 'activation': 'ReLU', 'n_layers': 3, 'n_neurons': 88, 'final_layer': 'Tanh', 'p_value': 1.0, 'lambda_repulsive': 0.1987511101778108}. Best is trial 41 with value: 2.067258947135997e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.004315: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [05:39<00:00,  5.89epoch/s]\n",
      "\u001b[32m[I 2023-02-01 03:02:34,691]\u001b[0m Trial 43 finished with value: 0.005328926723450422 and parameters: {'batch_size': 40, 'learning_rate': 0.0004056032368060803, 'beta1': 0.925, 'beta2': 0.9756793145774997, 'integral_points': 50, 'activation': 'ReLU', 'n_layers': 2, 'n_neurons': 52, 'final_layer': 'Tanh', 'p_value': 1.5, 'lambda_repulsive': 0.24556222729725258}. Best is trial 41 with value: 2.067258947135997e-06.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1999 ; tLoss: 0.000006: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [10:25<00:00,  3.20epoch/s]\n",
      "\u001b[32m[I 2023-02-01 03:13:00,684]\u001b[0m Trial 44 finished with value: 6.246526481845649e-06 and parameters: {'batch_size': 56, 'learning_rate': 0.00029688134215772954, 'beta1': 0.95, 'beta2': 0.9633350495150731, 'integral_points': 45, 'activation': 'ReLU', 'n_layers': 3, 'n_neurons': 64, 'final_layer': 'Tanh', 'p_value': 1.25, 'lambda_repulsive': 0.12718655372029114}. Best is trial 41 with value: 2.067258947135997e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.001261: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [06:47<00:00,  4.91epoch/s]\n",
      "\u001b[32m[I 2023-02-01 03:19:48,214]\u001b[0m Trial 45 finished with value: 0.0013443351490423083 and parameters: {'batch_size': 60, 'learning_rate': 0.00030542305187745386, 'beta1': 0.95, 'beta2': 0.9649800894680594, 'integral_points': 45, 'activation': 'ReLU', 'n_layers': 2, 'n_neurons': 64, 'final_layer': 'Tanh', 'p_value': 1.25, 'lambda_repulsive': 0.11384074342713306}. Best is trial 41 with value: 2.067258947135997e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000163: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [17:43<00:00,  1.88epoch/s]\n",
      "\u001b[32m[I 2023-02-01 03:37:32,216]\u001b[0m Trial 46 finished with value: 2.0933715859428048e-05 and parameters: {'batch_size': 88, 'learning_rate': 0.00021578984408357794, 'beta1': 0.95, 'beta2': 0.9691387199235599, 'integral_points': 50, 'activation': 'ReLU', 'n_layers': 3, 'n_neurons': 40, 'final_layer': 'Tanh', 'p_value': 1.0, 'lambda_repulsive': 0.13013696436210467}. Best is trial 41 with value: 2.067258947135997e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.015977: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [11:15<00:00,  2.96epoch/s]\n",
      "\u001b[32m[I 2023-02-01 03:48:47,623]\u001b[0m Trial 47 finished with value: 8.52231005410431e-06 and parameters: {'batch_size': 48, 'learning_rate': 0.0002709990754393619, 'beta1': 0.925, 'beta2': 0.9619308458017073, 'integral_points': 45, 'activation': 'ReLU', 'n_layers': 4, 'n_neurons': 28, 'final_layer': 'Sigmoid', 'p_value': 1.25, 'lambda_repulsive': 0.10207988686220743}. Best is trial 41 with value: 2.067258947135997e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.000008: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [28:39<00:00,  1.16epoch/s]\n",
      "\u001b[32m[I 2023-02-01 04:17:27,092]\u001b[0m Trial 48 finished with value: 6.9004609031253494e-06 and parameters: {'batch_size': 100, 'learning_rate': 0.00017534600292089174, 'beta1': 0.95, 'beta2': 0.9762551283393813, 'integral_points': 45, 'activation': 'ReLU', 'n_layers': 5, 'n_neurons': 52, 'final_layer': 'Tanh', 'p_value': 1.0, 'lambda_repulsive': 0.1491813958909684}. Best is trial 41 with value: 2.067258947135997e-06.\u001b[0m\n",
      "Train Epoch: 1999 ; tLoss: 0.001298: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [39:08<00:00,  1.17s/epoch]\n",
      "\u001b[32m[I 2023-02-01 04:56:35,980]\u001b[0m Trial 49 finished with value: 0.0007149120792746544 and parameters: {'batch_size': 60, 'learning_rate': 0.0004274046533026382, 'beta1': 0.8500000000000001, 'beta2': 0.989304500482117, 'integral_points': 50, 'activation': 'ReLU', 'n_layers': 4, 'n_neurons': 88, 'final_layer': 'Sigmoid', 'p_value': 3.0, 'lambda_repulsive': 0.17901539540817568}. Best is trial 41 with value: 2.067258947135997e-06.\u001b[0m\n",
      "/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/neptune_optuna/impl/__init__.py:475: FutureWarning:\n",
      "\n",
      "system_attrs has been deprecated in v3.1.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.1.0.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 1054 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 1054 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/zulap/2to2scattering/e/TOS-1015\n"
     ]
    }
   ],
   "source": [
    "# Start the Neptune run\n",
    "run = neptune.init_run(project=NEPTUNE_PROJECT, api_token=NEPTUNE_API_TOKEN)\n",
    "tags = {'Optimization': 'Full', 'optimizer': 'Adam', 'Huber': '0.1'}\n",
    "run[\"sys/tags\"].add(list(tags.values()))\n",
    "\n",
    "# Start the callback for Optuna\n",
    "#neptune_callback = optuna_utils.NeptuneCallback(run,log_plot_contour=False)\n",
    "\n",
    "# Start the parameter study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "#study.optimize(objective, n_trials=10, callbacks=[neptune_callback])\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Log Optuna Study metadata\n",
    "optuna_utils.log_study_metadata(study, run, log_plot_contour=False)\n",
    "    \n",
    "# Make sure to kill the Neptune logger run\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b45f779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 1 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/zulap/2to2scattering/e/TOS-1017\n"
     ]
    }
   ],
   "source": [
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e21c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective2(trial):\n",
    "    \n",
    "    params = {'batch_size': trial.suggest_int(\"batch_size\", 28, 128, 10),\n",
    "             'epochs_num': 2500,\n",
    "             'learning_rate': trial.suggest_float(\"learning_rate\", 0.0005, 0.0075),\n",
    "             'lr_scheduler': False, \n",
    "             'beta1' : 0.9,\n",
    "             'beta2' : 0.999,\n",
    "             'integral_points': trial.suggest_int(\"integral_points\", 20, 50, 5),\n",
    "             'method_int': 'trapz',\n",
    "             'activation': 'ReLU',\n",
    "             'fund_dom': False,\n",
    "             'n_layers': trial.suggest_int(\"n_layers\", 3, 5),\n",
    "             'n_neurons': trial.suggest_int(\"n_neurons\", 16, 96, 8),\n",
    "             'final_layer': 'Tanh',\n",
    "             'loss': 'MSE',\n",
    "             'scaled_loss': False,\n",
    "             'scan_param': 1,\n",
    "             'bound_phi': math.pi,\n",
    "             'num_nets': 2,\n",
    "             'p_value': trial.suggest_float(\"p_value\", 1, 2, step=0.25),\n",
    "             'lambda_repulsive': trial.suggest_float(\"lambda_repulsive\", 0.1, 3)}\n",
    "    \n",
    "    params['layer_list'] = params['n_layers'] * [params['n_neurons']]\n",
    "    \n",
    "    crichtonnet = ModuleInterpCrichton(lambda_param=params['scan_param'])\n",
    "    netsolver = train_run(params, device, module_net=crichtonnet, seed_num=42, optim_name='Adam')\n",
    "    \n",
    "    eval_loss = eval_run_loss(netsolver, device)\n",
    "    \n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4176ac90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/zulap/2to2scattering/e/TOS-1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info (NVML): NVML Shared Library Not Found. GPU usage metrics may not be reported. For more information, see https://docs.neptune.ai/help/nvml_error/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-01 12:41:30,150]\u001b[0m A new study created in memory with name: no-name-ed10882a-e424-44d4-a8bb-477e0661c3de\u001b[0m\n",
      "Train Epoch: 2499 ; tLoss: 0.000205: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2500/2500 [17:41<00:00,  2.36epoch/s]\n",
      "\u001b[32m[I 2023-02-01 12:59:12,241]\u001b[0m Trial 0 finished with value: 0.00019795855041593313 and parameters: {'batch_size': 88, 'learning_rate': 0.0018165671976861242, 'integral_points': 30, 'n_layers': 4, 'n_neurons': 40, 'p_value': 1.25, 'lambda_repulsive': 1.7068824136252545}. Best is trial 0 with value: 0.00019795855041593313.\u001b[0m\n",
      "Train Epoch: 2499 ; tLoss: 0.000857: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2500/2500 [26:36<00:00,  1.57epoch/s]\n",
      "\u001b[32m[I 2023-02-01 13:25:48,616]\u001b[0m Trial 1 finished with value: 0.001166922622360289 and parameters: {'batch_size': 68, 'learning_rate': 0.006796276340648397, 'integral_points': 45, 'n_layers': 4, 'n_neurons': 32, 'p_value': 2.0, 'lambda_repulsive': 1.740949840425609}. Best is trial 0 with value: 0.00019795855041593313.\u001b[0m\n",
      "Train Epoch: 76 ; tLoss: 0.188485:   3%|██▊                                                                                         | 77/2500 [02:41<1:24:52,  2.10s/epoch]\n",
      "\u001b[33m[W 2023-02-01 13:28:30,558]\u001b[0m Trial 2 failed with parameters: {'batch_size': 118, 'learning_rate': 0.0005629464735902466, 'integral_points': 50, 'n_layers': 4, 'n_neurons': 88, 'p_value': 1.5, 'lambda_repulsive': 2.7249206588857278} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/72/3_7s1hmx5b51pxyxq3_zkbxw0000gn/T/ipykernel_76071/3033423086.py\", line 27, in objective2\n",
      "    netsolver = train_run(params, device, module_net=crichtonnet, seed_num=42, optim_name='Adam')\n",
      "  File \"/var/folders/72/3_7s1hmx5b51pxyxq3_zkbxw0000gn/T/ipykernel_76071/997265251.py\", line 48, in train_run\n",
      "    netsolver.train(run_neptune)\n",
      "  File \"/var/folders/72/3_7s1hmx5b51pxyxq3_zkbxw0000gn/T/ipykernel_76071/627721796.py\", line 293, in train\n",
      "    loss.backward()\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/torch/_tensor.py\", line 396, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/Users/aurelien/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 173, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-02-01 13:28:30,569]\u001b[0m Trial 2 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#study.optimize(objective, n_trials=10, callbacks=[neptune_callback])\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective2, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Log Optuna Study metadata\u001b[39;00m\n\u001b[1;32m     15\u001b[0m optuna_utils\u001b[38;5;241m.\u001b[39mlog_study_metadata(study, run, log_plot_contour\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn [7], line 27\u001b[0m, in \u001b[0;36mobjective2\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     24\u001b[0m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_list\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m [params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_neurons\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     26\u001b[0m crichtonnet \u001b[38;5;241m=\u001b[39m ModuleInterpCrichton(lambda_param\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscan_param\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 27\u001b[0m netsolver \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_net\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrichtonnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m eval_loss \u001b[38;5;241m=\u001b[39m eval_run_loss(netsolver, device)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eval_loss\n",
      "Cell \u001b[0;32mIn [4], line 48\u001b[0m, in \u001b[0;36mtrain_run\u001b[0;34m(parameters, device, run_neptune, module_net, seed_num, optim_name)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Create the simulation, train it and return the trained result\u001b[39;00m\n\u001b[1;32m     47\u001b[0m netsolver \u001b[38;5;241m=\u001b[39m PhaseNetSolver(parameters, phinn, modnn, optimizer, random_gen_cpu, lr_scheduler, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mnetsolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_neptune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m netsolver\n",
      "Cell \u001b[0;32mIn [3], line 293\u001b[0m, in \u001b[0;36mPhaseNetSolver.train\u001b[0;34m(self, neptune_run)\u001b[0m\n\u001b[1;32m    291\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomplete_loss(zpoints, dual_active, logger\u001b[38;5;241m=\u001b[39mneptune_run)\n\u001b[1;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_allocated(device)\n\u001b[0;32m--> 293\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m neptune_run \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     neptune_run[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics/learning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/spinorhelicity/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start the Neptune run\n",
    "run = neptune.init_run(project=NEPTUNE_PROJECT, api_token=NEPTUNE_API_TOKEN)\n",
    "tags = {'Optimization': 'Full', 'optimizer': 'Adam', 'Huber': '0.1'}\n",
    "run[\"sys/tags\"].add(list(tags.values()))\n",
    "\n",
    "# Start the callback for Optuna\n",
    "#neptune_callback = optuna_utils.NeptuneCallback(run,log_plot_contour=False)\n",
    "\n",
    "# Start the parameter study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "#study.optimize(objective, n_trials=10, callbacks=[neptune_callback])\n",
    "study.optimize(objective2, n_trials=50)\n",
    "\n",
    "# Log Optuna Study metadata\n",
    "optuna_utils.log_study_metadata(study, run, log_plot_contour=True)\n",
    "    \n",
    "# Make sure to kill the Neptune logger run\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/zulap/2to2scattering/e/TOS-1033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info (NVML): NVML Shared Library Not Found. GPU usage metrics may not be reported. For more information, see https://docs.neptune.ai/help/nvml_error/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 974 ; tLoss: 0.005758:  39%|███████████████████████████████████▉                                                        | 975/2500 [02:05<03:14,  7.82epoch/s]"
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(project=NEPTUNE_PROJECT, api_token=NEPTUNE_API_TOKEN)\n",
    "tags = {'Optimization': 'Crichton', 'optimizer': 'Adam', 'Huber': '0.1'}\n",
    "run[\"sys/tags\"].add(list(tags.values()))\n",
    "params = {'batch_size': 64,\n",
    "         'epochs_num': 2500,\n",
    "         'learning_rate': 0.001,\n",
    "         'lr_scheduler': False, \n",
    "         'beta1' : 0.9,\n",
    "         'beta2' : 0.999,\n",
    "         'integral_points': 25,\n",
    "         'method_int': 'trapz',\n",
    "         'activation': 'ReLU',\n",
    "         'fund_dom': False,\n",
    "         'n_layers': 4,\n",
    "         'n_neurons': 64,\n",
    "         'final_layer': 'Tanh',\n",
    "         'loss': 'MSE',\n",
    "         'scaled_loss': False,\n",
    "         'scan_param': 1,\n",
    "         'bound_phi': math.pi,\n",
    "         'num_nets': 2,\n",
    "         'p_value': 2,\n",
    "         'lambda_repulsive': 1}\n",
    "\n",
    "params['layer_list'] = params['n_layers'] * [params['n_neurons']]\n",
    "\n",
    "crichtonnet = ModuleInterpCrichton(lambda_param=params['scan_param'])\n",
    "netsolver = train_run(params, device, module_net=crichtonnet, seed_num=42, optim_name='Adam')\n",
    "\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583bbe2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
